{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Name of student:** Rohan Karthikeyan  \n**Roll Number:** MDS202226\n\n## Basic imports\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pprint import pprint\npd.set_option('display.max_colwidth', 100)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-19T07:25:02.709254Z","iopub.execute_input":"2023-09-19T07:25:02.710109Z","iopub.status.idle":"2023-09-19T07:25:03.190105Z","shell.execute_reply.started":"2023-09-19T07:25:02.710052Z","shell.execute_reply":"2023-09-19T07:25:03.188440Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport string\nimport time\n\nimport wordcloud\nfrom nltk.corpus import stopwords\n\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.linear_model import LogisticRegression","metadata":{"execution":{"iopub.status.busy":"2023-09-19T07:25:03.194130Z","iopub.execute_input":"2023-09-19T07:25:03.195417Z","iopub.status.idle":"2023-09-19T07:25:04.204027Z","shell.execute_reply.started":"2023-09-19T07:25:03.195358Z","shell.execute_reply":"2023-09-19T07:25:04.202727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Working on the Enron spam subset","metadata":{}},{"cell_type":"code","source":"enron_spam = pd.read_csv('/kaggle/input/spamemails/enronSpamSubset.csv', usecols = [2, 3])\nenron_spam","metadata":{"execution":{"iopub.status.busy":"2023-09-19T07:25:04.205845Z","iopub.execute_input":"2023-09-19T07:25:04.210937Z","iopub.status.idle":"2023-09-19T07:25:04.465349Z","shell.execute_reply.started":"2023-09-19T07:25:04.210883Z","shell.execute_reply":"2023-09-19T07:25:04.464110Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have 10000 emails in our dataset. Let us now look at a sample email text:","metadata":{}},{"cell_type":"code","source":"enron_spam.iloc[3244, 0]","metadata":{"execution":{"iopub.status.busy":"2023-09-19T07:25:04.469500Z","iopub.execute_input":"2023-09-19T07:25:04.470592Z","iopub.status.idle":"2023-09-19T07:25:04.483884Z","shell.execute_reply.started":"2023-09-19T07:25:04.470536Z","shell.execute_reply":"2023-09-19T07:25:04.482170Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"One can observe that this is clearly spam.","metadata":{}},{"cell_type":"markdown","source":"## Cleaning the dataset using `regex`","metadata":{}},{"cell_type":"markdown","source":"We perform the following steps of cleaning:\n* Convert text into lowercase;\n* Remove new or extra lines;\n* Remove tabs, punctuation, and commas;\n* Remove extra white spaces; and\n* Remove stop words","metadata":{}},{"cell_type":"code","source":"stopword_list = stopwords.words('english')\nstopword_list.append('subject')  # A commonly repeating word in emails","metadata":{"execution":{"iopub.status.busy":"2023-09-19T07:25:04.486116Z","iopub.execute_input":"2023-09-19T07:25:04.486699Z","iopub.status.idle":"2023-09-19T07:25:04.496564Z","shell.execute_reply.started":"2023-09-19T07:25:04.486654Z","shell.execute_reply":"2023-09-19T07:25:04.494921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_text(text):\n    text = text.lower()  # Lowercase words\n    text = re.sub(r\"\\n+\", \" \", text)  # Remove new line\n\n    # Remove tabs, punctuation and commas\n    text = re.sub(\"[\" + string.punctuation + \"]\", \" \", text)\n    text = re.sub(\"\\s+\", \" \", text) # Remove extra white space\n\n    # Remove stopwords from text\n    text = \" \".join([word for word in text.split() if word not in stopword_list])\n    return text","metadata":{"execution":{"iopub.status.busy":"2023-09-19T07:25:04.498681Z","iopub.execute_input":"2023-09-19T07:25:04.499453Z","iopub.status.idle":"2023-09-19T07:25:04.512760Z","shell.execute_reply.started":"2023-09-19T07:25:04.499214Z","shell.execute_reply":"2023-09-19T07:25:04.510945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start = time.time()\nenron_spam['Body'] = enron_spam['Body'].apply(lambda x : preprocess_text(x))\nend = time.time()\nprint('Time to execute: {:.3f} secs.\\n'.format(end - start))\n\n# Display a few sample texts\ndisplay(enron_spam.sample(10))","metadata":{"execution":{"iopub.status.busy":"2023-09-19T07:25:04.515056Z","iopub.execute_input":"2023-09-19T07:25:04.515580Z","iopub.status.idle":"2023-09-19T07:25:13.277115Z","shell.execute_reply.started":"2023-09-19T07:25:04.515538Z","shell.execute_reply":"2023-09-19T07:25:13.276158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the number of spam and non-spam emails\npd.DataFrame(enron_spam['Label'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2023-09-19T07:25:13.278701Z","iopub.execute_input":"2023-09-19T07:25:13.279491Z","iopub.status.idle":"2023-09-19T07:25:13.292838Z","shell.execute_reply.started":"2023-09-19T07:25:13.279452Z","shell.execute_reply":"2023-09-19T07:25:13.290918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Display a wordcloud of the email payloads\n\nWe display the 100 most common words found in the email payloads.","metadata":{}},{"cell_type":"code","source":"content = ' '.join(enron_spam['Body'].values)\nfig, ax = plt.subplots(figsize = (14, 10))\nwc = wordcloud.WordCloud(width = 800, height = 600, max_words = 100, stopwords = stopword_list).generate(content)\nax.imshow(wc)\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-19T07:25:13.294854Z","iopub.execute_input":"2023-09-19T07:25:13.295352Z","iopub.status.idle":"2023-09-19T07:25:31.774952Z","shell.execute_reply.started":"2023-09-19T07:25:13.295316Z","shell.execute_reply":"2023-09-19T07:25:31.773809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Observations:** \n1. One can see that the word `enron` occurs frequently, as would be expected. \n2. The word `ect` also occurs frequently; it stands for $\\text{Enron Capital and Trade Resources}$ and was Enron's trading and financial arm. ","metadata":{}},{"cell_type":"markdown","source":"## Building a Logistic regression classifier","metadata":{}},{"cell_type":"markdown","source":"Before we embark on our goal of spam classification, it is important to first extract features from the text. \nThis is done by converting the raw text into a vector representation.\n\nThere are two main functions for doing the required task:\n1. `CountVectorizer`: It converts a collection of text documents to a matrix of ***token counts***.\n2. `TfidfVectorizer`: It converts a collection of text documents to a matrix of ***TF-IDF features***.\n\nMore details will be given under the respective sections.\n\nAdditionally, we report the **odds** by interpreting the coefficients of the top 15 features, as recognized by the `LogisticRegression`\nclassifier. The model reports log odds, which we can convert to odds, by taking the exponential: $e^{\\text{log odds}}$.","metadata":{}},{"cell_type":"code","source":"X_raw = enron_spam['Body'].copy()\ny = enron_spam['Label'].copy()\n\nX_train_raw, X_test_raw, y_train, y_test = train_test_split(X_raw, y, test_size = 0.20, random_state = 49)\nX_train_raw","metadata":{"execution":{"iopub.status.busy":"2023-09-19T07:25:31.776159Z","iopub.execute_input":"2023-09-19T07:25:31.776584Z","iopub.status.idle":"2023-09-19T07:25:31.797085Z","shell.execute_reply.started":"2023-09-19T07:25:31.776550Z","shell.execute_reply":"2023-09-19T07:25:31.795487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CountVectorizer aka Bag-of-words\n\nThis does two very simple things to convert text to numerical feature vectors:\n1. It tokenizes the strings (either by word or by characters) and assigns each token an unique ID.\n2. It counts the number of times each token occurs in the document.\n\nThe `scikit-learn` implementation comes with a number of useful arguments to the function, which we make use of.\nOf importance to us is the `ngram_range` argument. We create a helper function that helps us change this argument and check the performance.\n\n","metadata":{}},{"cell_type":"code","source":"def perform_train_test(vectorizer, arguments):\n    \"\"\"Instantiate a Vectorizer object and perform training and testing.\"\"\"\n    for k, v in arguments.items():\n        setattr(vectorizer, k, v)\n    print('Vectorizer used: {}\\n'.format(vectorizer))\n    X_train = vectorizer.fit_transform(X_train_raw)\n\n    classifier = LogisticRegression(class_weight = 'balanced')\n    classifier.fit(X_train, y_train)\n\n    X_test = vectorizer.transform(X_test_raw)\n    y_pred_uni = classifier.predict(X_test)\n\n    # Note that 0 == Not spam\n    print('-'*50)\n    print(classification_report(y_test, y_pred_uni, target_names = ['Not spam', 'Spam']))\n\n    # Get top 15 features with the odds ratio\n    print_odds_ratio(vectorizer, classifier)\n\n\ndef print_odds_ratio(vectorizer, classifier, num_features=15):\n    \"\"\"Print the odds ratio of the top `num_features` features of the classifier.\"\"\"\n    # Get mapping of terms to feature indices\n    learned_vocab = vectorizer.vocabulary_\n\n    # Obtain coefficient of features in the decision fn.\n    coeffs = classifier.coef_[0]\n\n    # Get indices of 15 largest elements\n    indices = np.argpartition(coeffs, -num_features)[-num_features:]\n\n    # Search for the `indices` in the learned vocabulary\n    # First create a new dict from the old dict reversing the keys and values\n    new = dict(zip(learned_vocab.values(), learned_vocab.keys()))\n\n    # Calculate odds\n    odds = {new[index]: np.round(np.exp(coeffs[index]), decimals=4) for index in indices}\n    sorted_odds = sorted(odds.items(), key=lambda x:x[1])\n    odds_ratio = dict(sorted_odds)  # Sort by value (asc.)\n    print('-'*50)\n    print('Odds ratio of the top {} features are:'.format(num_features))\n    pprint(odds_ratio, width=1)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T07:25:31.798991Z","iopub.execute_input":"2023-09-19T07:25:31.799538Z","iopub.status.idle":"2023-09-19T07:25:31.814962Z","shell.execute_reply.started":"2023-09-19T07:25:31.799495Z","shell.execute_reply":"2023-09-19T07:25:31.813504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Model 1: Only unigrams\ncnt_vectorizer = CountVectorizer()\ncnt_args = {'min_df': 5, 'max_features': 6000, 'ngram_range': (1, 1)}\nperform_train_test(cnt_vectorizer, cnt_args)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T07:25:31.816876Z","iopub.execute_input":"2023-09-19T07:25:31.817410Z","iopub.status.idle":"2023-09-19T07:25:34.830919Z","shell.execute_reply.started":"2023-09-19T07:25:31.817349Z","shell.execute_reply":"2023-09-19T07:25:34.829368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Model 2: Only bigrams\ncnt_args['ngram_range'] = (2, 2)\nperform_train_test(cnt_vectorizer, cnt_args)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T07:25:34.836036Z","iopub.execute_input":"2023-09-19T07:25:34.836529Z","iopub.status.idle":"2023-09-19T07:25:41.766967Z","shell.execute_reply.started":"2023-09-19T07:25:34.836491Z","shell.execute_reply":"2023-09-19T07:25:41.765736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Model 3: Unigrams and bigrams\ncnt_args['ngram_range'] = (1, 2)\nperform_train_test(cnt_vectorizer, cnt_args)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T07:25:41.768617Z","iopub.execute_input":"2023-09-19T07:25:41.769333Z","iopub.status.idle":"2023-09-19T07:25:50.694474Z","shell.execute_reply.started":"2023-09-19T07:25:41.769294Z","shell.execute_reply":"2023-09-19T07:25:50.693124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Inferences:** \n\n1. There is a ***pronounced drop*** in recall when using only bigrams.\n2. In terms of predictive precision, there is ***no difference*** in using both unigrams and bigrams as opposed to using only unigrams.\n3. There is a slight difference in the top 15 features recognized by the unigram-bigram model v/s the unigram model.\n4. While some of the top features detected clearly increases the odds of the email being spam (e.g., *paliourg*, *click*), some features could also occur in non-spam emails (e.g. *software*, *mobile*).","metadata":{}},{"cell_type":"markdown","source":"### TfidfVectorizer aka Term-frequency inverse document frequency\n\nWhile `CountVectorizer` only takes into account the number of times a word appears in the document, the `TfidfVectorizer` also takes\ninto account not only the number of occurrences of a word in the document but also how important that word is to the corpus.","metadata":{}},{"cell_type":"code","source":"## Model 1: Only unigrams\ntfidf_vectorizer = TfidfVectorizer()\ntfidf_args = {'min_df': 5, 'max_features': 6000, 'ngram_range': (1, 1)}\nperform_train_test(tfidf_vectorizer, tfidf_args)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T07:25:50.696460Z","iopub.execute_input":"2023-09-19T07:25:50.697337Z","iopub.status.idle":"2023-09-19T07:25:53.624491Z","shell.execute_reply.started":"2023-09-19T07:25:50.697292Z","shell.execute_reply":"2023-09-19T07:25:53.622559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Model 2: Only bigrams\ntfidf_args['ngram_range'] = (2, 2)\nperform_train_test(tfidf_vectorizer, tfidf_args)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T07:25:53.626461Z","iopub.execute_input":"2023-09-19T07:25:53.626917Z","iopub.status.idle":"2023-09-19T07:26:00.070371Z","shell.execute_reply.started":"2023-09-19T07:25:53.626878Z","shell.execute_reply":"2023-09-19T07:26:00.069135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Model 3: Unigrams and bigrams\ntfidf_args['ngram_range'] = (1, 2)\nperform_train_test(tfidf_vectorizer, tfidf_args)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T07:26:00.072206Z","iopub.execute_input":"2023-09-19T07:26:00.072618Z","iopub.status.idle":"2023-09-19T07:26:08.979854Z","shell.execute_reply.started":"2023-09-19T07:26:00.072586Z","shell.execute_reply":"2023-09-19T07:26:08.977628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Inferences:** \n\n1. The performance of `TfidfVectorizer` is **very similar** to the `CountVectorizer` in all three cases.\n2. *Some* differences are observed between the corresponding top features list b/w the `TfidfVectorizer` and `CountVectorizer`.\n3. The odds are higher than in the previous model, indicating that the classifier ***strongly*** believes the presence of one (or more) these features makes the email likely to be spam.\n4. There is ***no difference*** in the top 15 features recognized by the unigram-bigram model v/s the unigram model.\n5. The feature with the ***highest*** odds in two variants starts with *http*: a hyperlink! It's the second top feature in the bigram model too.","metadata":{}}]}